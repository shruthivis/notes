<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Strict//EN">
<html>
<head>

  <meta content="text/html; charset=ISO-8859-1" http-equiv="content-type">
  <title>Research article notes</title>


  <link rel="stylesheet" href="../../contentStylesheet.css" type="text/css">

</head>


<body>

<p><a href="http://rsise.anu.edu.au/%7Ewilliams/papers/P132.pdf">Estimating
the Support of a High-Dimensional Distribution,</a> Scholkopf
etal
</p>

<h4>The review, in latex:</h4>
<pre><br>\documentclass[10pt]{amsart}<br>\input{../../macros}<br>%opening<br>\title{Estimating the Support of a High-Dimensional Distribution: Scholkopf et al}<br>\author{vishvAs vAsuki}<br>\begin{document}<br>\maketitle<br>\section{Depth with which this was read, progress}<br>Did not read and understand sections 2, 4, parts of 5, parts of 7 and appendix A well.<br>\section{Problems}<br>Given a sample drawn from a distribution drawn from a high dimensional space, how would you estimate its support?<br>\section{Motivating real life scenarios}<br>Novelty detection. They try out the example on the USPS hand-written digit database and seem to succeed in identifying outliers<br>\section{The Model}<br>\subsection{Terms and Variables used}<br>\subsection{The specification of the problem}<br>Find set S such that $Pr(x \notin S)<p \in="" (0,1="" can="" be="" solved="" by="" probability="" density="" estimation="" techniques="" but="" actually="" simpler="" \section{results="" methods="" and="" ideas="" \subsection{using="" corresponding="" given="" examples="" $\set{s_{i="" project="" feature="" space="" associated="" kernel="" $k(x,y="\gf(x)^{T}\gf(y)$;" find="" $w^{t}\gf(x="" all="" points="" in="" one="" of="" hyperplane="" outliers="" fall="" on="" other="" side="" so="" allowing="" a="" soft="" margin="" want="" solve="" $\max_{\gr="" \frac{\gr}{\norm{w="" c\sum="" such="" that="" $w^{t}\gf(x_{i="" $\equiv="" obj="" fn="" $\min_{w="" \norm{w}^{2}/2="" some="" coefficient="" \gn="" 1="" lagrangian="" $l(w="" \ga="" \ga_{i}(w^{t}\gf(x_{i="" +="" \gx_{i="" \sum="" \gb_{i}\gx_{i="" $\ga="" \gb="\norm{w}^{2}/2" \geq="" set="" derivatives="" wrt="" primal="" vars="" \gx="" get="" $w="" \frac{1}{\gn="" n="" \sum_{i="" thence="" the="" support="" identifier="" $f="sgn(w^{T}x" \gr="" dual="" optimization="" problem="" becomes="" \="" $min_{\ga="" -="" 2^{-1="" \sum_{i,j}a_{i}a_{j}k(x_{i="" x_{j="" subject="" to="" $0="" \ga_{i="\frac{1}{\gn" \leq="" (\gn="" n)^{-1="" \sum_{i}\ga_{i="1$." solving="" this="" discover="" w="" then="" recover="" using="" $\gr="" for="" $x_{i="" $\ga_{i="" \gb_{i="" \neq="" 0="" (support="" vector="" with="" $\gb_{i=""> 0$): $\exists x_{i}$ as $\sum \ga_{i} = 1; \ga_{i} \geq 0$.<br>\subsubsection{Choosing kernel, tuning parameters}<br>$\gn \propto$ softness of the margin, number of support vectors, thence the runtime, sensitivity to appearence of novelty.<br>With Gaussian kernel, any data set is seperable as everything is mapped to same quadrant in feature space.<br>\subsubsection{Comparison with thresholded Kernel Density estimator}<br>If $\gn = 1, \ga_{i} = 1/N$, support identifier $f = sgn(\sum_{i}\ga_{i}k(x_{i},x) - \gr)$ same as one using a Kernel (Parzen) Density estimator. What happens when $\gn &lt; 1$?<br>\subsubsection{Comparison with using soft margin balls in the feature space}<br>Here one solves: \\<br>$\min_{R, \gx, c} R^{2} + \frac{1}{\gn N} \sum_{i}\gx_{i}$ subject to $\norm{\gf(x_{i} - c)}^{2} - \gx_{i}\leq R^{2}, \gx_{i}\geq 0$.<br>After using the Lagrangian, finding the critical points and substituting, this leads to the dual $\min_{\ga}\sum_{i,j}\ga_{i}\ga_{j}k(x_{i}, x_{j}) - \sum_{i}\ga_{i}k(x_{i}, x_{i})$ subject to $0 \leq \ga_{i} \leq \frac{1}{\gn N}, \sum \ga_{i} = 1$, and the solution $c = \sum \ga_{i}\gf(x_{i})$ corresponding to support identifier $f = sgn(R^{2} - \sum_{i,j}\ga_{i}\ga_{j}k(x_{i}, x_{j}) + 2\sum_{i}k(x_{i}, x)- k(x,x) )$ \chk.<br>For homogenous kernels, $k(x,x)$ is a constant and the dual minimization problem and the support identifier is equivalent to the minimization problem derived from the hyperplane formulation. So, all mapped patterns lie in a sphere in feature space; finding the smallest sphere containing them is equivalent to finding the segment of the sphere containing the data points, which reduces to finding the separating hyperplane.<br>\subsubsection{Connection to binary classification}<br>Hyperplane $(w, \gr = 0)$ \\<br>separates $\set{(x_{i}, 1)}$ from $(-x_{i}, -1)$ with margin $\gr/\norm{w}$ and vice-versa.<br>\section{Assumptions}<br>\section{What could have been tried to yield equivalent results}<br>\section{Open problems}<br>\oprob How to decide width of Gaussian kernel to use? Can you use information about the abnormal class in choosing the kernel?<br>\section{Interesting facts and results from elsewhere}<br>\section{Comments on writing style}<br>Well written. Some graphs could have used better explanation.<br>\section{Questions}<br>\end{document}<br></p></pre>

</body>
</html>
