\BOOKMARK [0][-]{section*.1}{Contents}{}
\BOOKMARK [-1][-]{part.1}{Introduction}{}
\BOOKMARK [0][-]{chapter.1}{Modeling}{part.1}
\BOOKMARK [1][-]{section.1.1}{Model/ hypothesize for predictive ability}{chapter.1}
\BOOKMARK [2][-]{subsection.1.1.1}{Realism vs conciseness}{section.1.1}
\BOOKMARK [2][-]{subsection.1.1.2}{Frequentist vs Subjective interpretations}{section.1.1}
\BOOKMARK [2][-]{subsection.1.1.3}{Sampling mechanism}{section.1.1}
\BOOKMARK [3][-]{subsubsection.1.1.3.1}{Possible errors}{subsection.1.1.3}
\BOOKMARK [1][-]{section.1.2}{Using the models}{chapter.1}
\BOOKMARK [2][-]{subsection.1.2.1}{Effectiveness of simple models}{section.1.2}
\BOOKMARK [0][-]{chapter.2}{Non probabilistic models}{part.1}
\BOOKMARK [-1][-]{part.2}{Simple Random variable densities}{}
\BOOKMARK [0][-]{chapter.3}{Distribution of values}{part.2}
\BOOKMARK [1][-]{section.3.1}{Specification and classes}{chapter.3}
\BOOKMARK [2][-]{subsection.3.1.1}{Notation}{section.3.1}
\BOOKMARK [2][-]{subsection.3.1.2}{Parameter types}{section.3.1}
\BOOKMARK [2][-]{subsection.3.1.3}{Specify continuous distribution over bounded support}{section.3.1}
\BOOKMARK [1][-]{section.3.2}{Inference, Sampling from distribution}{chapter.3}
\BOOKMARK [0][-]{chapter.4}{Discrete probability distributions}{part.2}
\BOOKMARK [1][-]{section.4.1}{Coin toss distribution}{chapter.4}
\BOOKMARK [2][-]{subsection.4.1.1}{Properties}{section.4.1}
\BOOKMARK [2][-]{subsection.4.1.2}{Odds of success}{section.4.1}
\BOOKMARK [1][-]{section.4.2}{Multiple coin-toss}{chapter.4}
\BOOKMARK [2][-]{subsection.4.2.1}{Properties}{section.4.2}
\BOOKMARK [2][-]{subsection.4.2.2}{Approximations}{section.4.2}
\BOOKMARK [3][-]{subsubsection.4.2.2.1}{With exponential decay for sq deviation distribution}{subsection.4.2.2}
\BOOKMARK [2][-]{subsection.4.2.3}{Poisson distribution}{section.4.2}
\BOOKMARK [2][-]{subsection.4.2.4}{Random walk on a line}{section.4.2}
\BOOKMARK [2][-]{subsection.4.2.5}{\(balls, bins\)}{section.4.2}
\BOOKMARK [3][-]{subsubsection.4.2.5.1}{Process}{subsection.4.2.5}
\BOOKMARK [3][-]{subsubsection.4.2.5.2}{Distribution}{subsection.4.2.5}
\BOOKMARK [1][-]{section.4.3}{Categorical distribution}{chapter.4}
\BOOKMARK [1][-]{section.4.4}{Multinomial distribution}{chapter.4}
\BOOKMARK [1][-]{section.4.5}{Geometric distribution}{chapter.4}
\BOOKMARK [1][-]{section.4.6}{Hypergeometric distribution}{chapter.4}
\BOOKMARK [1][-]{section.4.7}{Smoothing}{chapter.4}
\BOOKMARK [2][-]{subsection.4.7.1}{Motivation}{section.4.7}
\BOOKMARK [3][-]{subsubsection.4.7.1.1}{Incomplete knowledge of range}{subsection.4.7.1}
\BOOKMARK [3][-]{subsubsection.4.7.1.2}{Assumption of continuous ran\(X\)}{subsection.4.7.1}
\BOOKMARK [2][-]{subsection.4.7.2}{Add 1}{section.4.7}
\BOOKMARK [3][-]{subsubsection.4.7.2.1}{Add k}{subsection.4.7.2}
\BOOKMARK [2][-]{subsection.4.7.3}{Different additions}{section.4.7}
\BOOKMARK [3][-]{subsubsection.4.7.3.1}{Use backoff probabilities}{subsection.4.7.3}
\BOOKMARK [0][-]{chapter.5}{Mode-deviation penalizers}{part.2}
\BOOKMARK [1][-]{section.5.1}{Inverse squared decay}{chapter.5}
\BOOKMARK [2][-]{subsection.5.1.1}{Limited to positive deviation}{section.5.1}
\BOOKMARK [0][-]{chapter.6}{Exponential families}{part.2}
\BOOKMARK [1][-]{section.6.1}{Exponential family of distributions}{chapter.6}
\BOOKMARK [2][-]{subsection.6.1.1}{Generated by h and feature function, parametrized by t}{section.6.1}
\BOOKMARK [3][-]{subsubsection.6.1.1.1}{Canonical form.}{subsection.6.1.1}
\BOOKMARK [3][-]{subsubsection.6.1.1.2}{Minimal parametrization.}{subsection.6.1.1}
\BOOKMARK [2][-]{subsection.6.1.2}{Undirected graphical model from exp family distribution}{section.6.1}
\BOOKMARK [2][-]{subsection.6.1.3}{Maximum entropy distribution with given means}{section.6.1}
\BOOKMARK [3][-]{subsubsection.6.1.3.1}{The optimization problem}{subsection.6.1.3}
\BOOKMARK [3][-]{subsubsection.6.1.3.2}{Lagrangian form}{subsection.6.1.3}
\BOOKMARK [3][-]{subsubsection.6.1.3.3}{Closest distribution to h with given means}{subsection.6.1.3}
\BOOKMARK [3][-]{subsubsection.6.1.3.4}{Parametrization by means}{subsection.6.1.3}
\BOOKMARK [1][-]{section.6.2}{Inverse Exponential decay for squared deviation from mean}{chapter.6}
\BOOKMARK [2][-]{subsection.6.2.1}{Importance}{section.6.2}
\BOOKMARK [2][-]{subsection.6.2.2}{1D case}{section.6.2}
\BOOKMARK [3][-]{subsubsection.6.2.2.1}{pdf, cdf}{subsection.6.2.2}
\BOOKMARK [3][-]{subsubsection.6.2.2.2}{Standard Normal distribution}{subsection.6.2.2}
\BOOKMARK [3][-]{subsubsection.6.2.2.3}{CDF calculation}{subsection.6.2.2}
\BOOKMARK [3][-]{subsubsection.6.2.2.4}{Moment generating function}{subsection.6.2.2}
\BOOKMARK [3][-]{subsubsection.6.2.2.5}{Other properties}{subsection.6.2.2}
\BOOKMARK [2][-]{subsection.6.2.3}{Multidimensional case}{section.6.2}
\BOOKMARK [3][-]{subsubsection.6.2.3.1}{Definition with univariates}{subsection.6.2.3}
\BOOKMARK [3][-]{subsubsection.6.2.3.2}{Distribution}{subsection.6.2.3}
\BOOKMARK [3][-]{subsubsection.6.2.3.3}{Covariance matrix is symmetric}{subsection.6.2.3}
\BOOKMARK [3][-]{subsubsection.6.2.3.4}{Geometric view}{subsection.6.2.3}
\BOOKMARK [3][-]{subsubsection.6.2.3.5}{Product of normal distributions}{subsection.6.2.3}
\BOOKMARK [2][-]{subsection.6.2.4}{.. dimensional Normal distribution}{section.6.2}
\BOOKMARK [2][-]{subsection.6.2.5}{Gaussian graphical models}{section.6.2}
\BOOKMARK [3][-]{subsubsection.6.2.5.1}{Uncorrelated variables}{subsection.6.2.5}
\BOOKMARK [1][-]{section.6.3}{1D distributions from exponential family}{chapter.6}
\BOOKMARK [2][-]{subsection.6.3.1}{Polynomial rise with inverse exponential decay for largeness}{section.6.3}
\BOOKMARK [3][-]{subsubsection.6.3.1.1}{Exponential decay distribution ..}{subsection.6.3.1}
\BOOKMARK [0][-]{chapter.7}{Other density families}{part.2}
\BOOKMARK [1][-]{section.7.1}{Sampling distributions}{chapter.7}
\BOOKMARK [2][-]{subsection.7.1.1}{Standard normal square sum}{section.7.1}
\BOOKMARK [2][-]{subsection.7.1.2}{Student's t distribution with k degrees of freedom}{section.7.1}
\BOOKMARK [2][-]{subsection.7.1.3}{F distribution}{section.7.1}
\BOOKMARK [1][-]{section.7.2}{Heavy tailed distributions}{chapter.7}
\BOOKMARK [2][-]{subsection.7.2.1}{Power law distributions}{section.7.2}
\BOOKMARK [3][-]{subsubsection.7.2.1.1}{With exponential cutoff}{subsection.7.2.1}
\BOOKMARK [3][-]{subsubsection.7.2.1.2}{Zipf's law for resource usage}{subsection.7.2.1}
\BOOKMARK [1][-]{section.7.3}{Mixture distribution}{chapter.7}
\BOOKMARK [1][-]{section.7.4}{Other pdf's}{chapter.7}
\BOOKMARK [2][-]{subsection.7.4.1}{Uniform and triangular distributions}{section.7.4}
\BOOKMARK [2][-]{subsection.7.4.2}{Log normal distribution}{section.7.4}
\BOOKMARK [2][-]{subsection.7.4.3}{Gumbel distribution}{section.7.4}
\BOOKMARK [2][-]{subsection.7.4.4}{Probability simplex coordinate powering}{section.7.4}
\BOOKMARK [3][-]{subsubsection.7.4.4.1}{2-dim case}{subsection.7.4.4}
\BOOKMARK [2][-]{subsection.7.4.5}{Wigner semicircle distribution}{section.7.4}
\BOOKMARK [-1][-]{part.3}{Model dependence among random variables}{}
\BOOKMARK [0][-]{chapter.8}{Distribution models}{part.3}
\BOOKMARK [1][-]{section.8.1}{Discrete L: Response probability: Discriminative models}{chapter.8}
\BOOKMARK [2][-]{subsection.8.1.1}{Boolean valued functions}{section.8.1}
\BOOKMARK [2][-]{subsection.8.1.2}{Probability from regression models}{section.8.1}
\BOOKMARK [3][-]{subsubsection.8.1.2.1}{Advantages of modeling probability}{subsection.8.1.2}
\BOOKMARK [2][-]{subsection.8.1.3}{Model numeric labels with regression models}{section.8.1}
\BOOKMARK [3][-]{subsubsection.8.1.3.1}{Dependence on choice of ran\(Y\)}{subsection.8.1.3}
\BOOKMARK [3][-]{subsubsection.8.1.3.2}{y in 1 of k binary encoding format}{subsection.8.1.3}
\BOOKMARK [2][-]{subsection.8.1.4}{Logistic model}{section.8.1}
\BOOKMARK [3][-]{subsubsection.8.1.4.1}{Log linear model for class probabilities}{subsection.8.1.4}
\BOOKMARK [3][-]{subsubsection.8.1.4.2}{Equivalent form: model log odds}{subsection.8.1.4}
\BOOKMARK [3][-]{subsubsection.8.1.4.3}{2-class case}{subsection.8.1.4}
\BOOKMARK [3][-]{subsubsection.8.1.4.4}{Risk factors interpretation}{subsection.8.1.4}
\BOOKMARK [3][-]{subsubsection.8.1.4.5}{As a linear discriminant}{subsection.8.1.4}
\BOOKMARK [2][-]{subsection.8.1.5}{Estimating parameters}{section.8.1}
\BOOKMARK [3][-]{subsubsection.8.1.5.1}{Sparsity of model parameters}{subsection.8.1.5}
\BOOKMARK [1][-]{section.8.2}{Discrete L: Response probability: Generative models}{chapter.8}
\BOOKMARK [2][-]{subsection.8.2.1}{Latent variable model}{section.8.2}
\BOOKMARK [2][-]{subsection.8.2.2}{Assume conditional independence of input variables}{section.8.2}
\BOOKMARK [3][-]{subsubsection.8.2.2.1}{Linear separator in some feature space}{subsection.8.2.2}
\BOOKMARK [3][-]{subsubsection.8.2.2.2}{Success in practice.}{subsection.8.2.2}
\BOOKMARK [3][-]{subsubsection.8.2.2.3}{Discriminative counterpart}{subsection.8.2.2}
\BOOKMARK [2][-]{subsection.8.2.3}{Use exponential family models}{section.8.2}
\BOOKMARK [3][-]{subsubsection.8.2.3.1}{Specification}{subsection.8.2.3}
\BOOKMARK [3][-]{subsubsection.8.2.3.2}{Tree structure assumptions}{subsection.8.2.3}
\BOOKMARK [1][-]{section.8.3}{Latent variable models: Expectation Maximization \(EM\) alg}{chapter.8}
\BOOKMARK [2][-]{subsection.8.3.1}{Problem}{section.8.3}
\BOOKMARK [3][-]{subsubsection.8.3.1.1}{Tough to Optimize likelihood}{subsection.8.3.1}
\BOOKMARK [3][-]{subsubsection.8.3.1.2}{Examples}{subsection.8.3.1}
\BOOKMARK [2][-]{subsection.8.3.2}{Iterative algorithm}{section.8.3}
\BOOKMARK [3][-]{subsubsection.8.3.2.1}{Intuition}{subsection.8.3.2}
\BOOKMARK [3][-]{subsubsection.8.3.2.2}{E-step}{subsection.8.3.2}
\BOOKMARK [3][-]{subsubsection.8.3.2.3}{M-step}{subsection.8.3.2}
\BOOKMARK [2][-]{subsection.8.3.3}{Analysis}{section.8.3}
\BOOKMARK [3][-]{subsubsection.8.3.3.1}{Maximizing an approximation of the likelihood}{subsection.8.3.3}
\BOOKMARK [3][-]{subsubsection.8.3.3.2}{Q\(w\) is a lower bound}{subsection.8.3.3}
\BOOKMARK [3][-]{subsubsection.8.3.3.3}{Convergence}{subsection.8.3.3}
\BOOKMARK [0][-]{chapter.9}{Graphical models}{part.3}
\BOOKMARK [1][-]{section.9.1}{Graphical model G of distribution}{chapter.9}
\BOOKMARK [2][-]{subsection.9.1.1}{The modeling problem}{section.9.1}
\BOOKMARK [3][-]{subsubsection.9.1.1.1}{Distribution structure/ sparsity}{subsection.9.1.1}
\BOOKMARK [3][-]{subsubsection.9.1.1.2}{Uses}{subsection.9.1.1}
\BOOKMARK [2][-]{subsection.9.1.2}{Factor graphs}{section.9.1}
\BOOKMARK [3][-]{subsubsection.9.1.2.1}{Factors of Pr\(x\)}{subsection.9.1.2}
\BOOKMARK [3][-]{subsubsection.9.1.2.2}{Conditional independence}{subsection.9.1.2}
\BOOKMARK [3][-]{subsubsection.9.1.2.3}{Expressiveness}{subsection.9.1.2}
\BOOKMARK [2][-]{subsection.9.1.3}{Undirected graphical models}{section.9.1}
\BOOKMARK [3][-]{subsubsection.9.1.3.1}{Factorization}{subsection.9.1.3}
\BOOKMARK [3][-]{subsubsection.9.1.3.2}{Conditional independence properties}{subsection.9.1.3}
\BOOKMARK [3][-]{subsubsection.9.1.3.3}{Tree structured case}{subsection.9.1.3}
\BOOKMARK [3][-]{subsubsection.9.1.3.4}{Pairwise graphical model}{subsection.9.1.3}
\BOOKMARK [3][-]{subsubsection.9.1.3.5}{Hierarchical models}{subsection.9.1.3}
\BOOKMARK [3][-]{subsubsection.9.1.3.6}{Discrete models}{subsection.9.1.3}
\BOOKMARK [2][-]{subsection.9.1.4}{Junction tree model}{section.9.1}
\BOOKMARK [3][-]{subsubsection.9.1.4.1}{Factorization}{subsection.9.1.4}
\BOOKMARK [2][-]{subsection.9.1.5}{Directed}{section.9.1}
\BOOKMARK [3][-]{subsubsection.9.1.5.1}{Extra notation}{subsection.9.1.5}
\BOOKMARK [3][-]{subsubsection.9.1.5.2}{Factorization}{subsection.9.1.5}
\BOOKMARK [3][-]{subsubsection.9.1.5.3}{Marginal independence}{subsection.9.1.5}
\BOOKMARK [3][-]{subsubsection.9.1.5.4}{Dependency seperation of X, Y by Z}{subsection.9.1.5}
\BOOKMARK [3][-]{subsubsection.9.1.5.5}{Other conditional independence properties}{subsection.9.1.5}
\BOOKMARK [3][-]{subsubsection.9.1.5.6}{Marginalized DAG}{subsection.9.1.5}
\BOOKMARK [2][-]{subsection.9.1.6}{Comparison}{section.9.1}
\BOOKMARK [3][-]{subsubsection.9.1.6.1}{Expressiveness}{subsection.9.1.6}
\BOOKMARK [3][-]{subsubsection.9.1.6.2}{Structural equivalence}{subsection.9.1.6}
\BOOKMARK [3][-]{subsubsection.9.1.6.3}{Independence relationships amongst vars}{subsection.9.1.6}
\BOOKMARK [1][-]{section.9.2}{Inference, decoding using Graphical model}{chapter.9}
\BOOKMARK [2][-]{subsection.9.2.1}{Problems}{section.9.2}
\BOOKMARK [3][-]{subsubsection.9.2.1.1}{Inference problems}{subsection.9.2.1}
\BOOKMARK [3][-]{subsubsection.9.2.1.2}{Decoding problem}{subsection.9.2.1}
\BOOKMARK [3][-]{subsubsection.9.2.1.3}{Evidence}{subsection.9.2.1}
\BOOKMARK [3][-]{subsubsection.9.2.1.4}{Solving for all variables}{subsection.9.2.1}
\BOOKMARK [2][-]{subsection.9.2.2}{Factorization and graph-based computations}{section.9.2}
\BOOKMARK [3][-]{subsubsection.9.2.2.1}{Benefit of factorization}{subsection.9.2.2}
\BOOKMARK [3][-]{subsubsection.9.2.2.2}{Graph traversal view}{subsection.9.2.2}
\BOOKMARK [2][-]{subsection.9.2.3}{Belief propagation}{section.9.2}
\BOOKMARK [3][-]{subsubsection.9.2.3.1}{The Bottom-up idea}{subsection.9.2.3}
\BOOKMARK [3][-]{subsubsection.9.2.3.2}{Node Elimination algorithm: Undirected Trees}{subsection.9.2.3}
\BOOKMARK [3][-]{subsubsection.9.2.3.3}{Reusing messages: Undirected Tree}{subsection.9.2.3}
\BOOKMARK [3][-]{subsubsection.9.2.3.4}{Tree Factor graphs}{subsection.9.2.3}
\BOOKMARK [3][-]{subsubsection.9.2.3.5}{General undirected graphs}{subsection.9.2.3}
\BOOKMARK [3][-]{subsubsection.9.2.3.6}{Approximate inference: Loopy belief propagation}{subsection.9.2.3}
\BOOKMARK [2][-]{subsection.9.2.4}{For Gaussian graphical models}{section.9.2}
\BOOKMARK [3][-]{subsubsection.9.2.4.1}{Connection with solving Ax = b}{subsection.9.2.4}
\BOOKMARK [2][-]{subsection.9.2.5}{Directed graphical models}{section.9.2}
\BOOKMARK [0][-]{chapter.10}{Sparse signal detection}{part.3}
\BOOKMARK [1][-]{section.10.1}{Scale mixture models}{chapter.10}
\BOOKMARK [0][-]{chapter.11}{Affinity modeling}{part.3}
\BOOKMARK [1][-]{section.11.1}{Problem}{chapter.11}
\BOOKMARK [2][-]{subsection.11.1.1}{Motivation}{section.11.1}
\BOOKMARK [1][-]{section.11.2}{Non probabilistic ways}{chapter.11}
\BOOKMARK [1][-]{section.11.3}{pLSA}{chapter.11}
\BOOKMARK [2][-]{subsection.11.3.1}{Aspect model}{section.11.3}
\BOOKMARK [2][-]{subsection.11.3.2}{Modeling assumptions}{section.11.3}
\BOOKMARK [2][-]{subsection.11.3.3}{Dimensionality reduction}{section.11.3}
\BOOKMARK [2][-]{subsection.11.3.4}{Defects}{section.11.3}
\BOOKMARK [1][-]{section.11.4}{Latent Dirichlet Allocation \(LDA\)}{chapter.11}
\BOOKMARK [0][-]{chapter.12}{Modeling stochastic processes}{part.3}
\BOOKMARK [1][-]{section.12.1}{Stochastic process with state space T}{chapter.12}
\BOOKMARK [2][-]{subsection.12.1.1}{Multiple coin toss processes}{section.12.1}
\BOOKMARK [2][-]{subsection.12.1.2}{Continuous time}{section.12.1}
\BOOKMARK [1][-]{section.12.2}{State transitions}{chapter.12}
\BOOKMARK [2][-]{subsection.12.2.1}{Assumptions about state transitions}{section.12.2}
\BOOKMARK [3][-]{subsubsection.12.2.1.1}{Dependence solely on prior state}{subsection.12.2.1}
\BOOKMARK [3][-]{subsubsection.12.2.1.2}{Dependence on prior k states}{subsection.12.2.1}
\BOOKMARK [3][-]{subsubsection.12.2.1.3}{Reduction to bigram state chain}{subsection.12.2.1}
\BOOKMARK [2][-]{subsection.12.2.2}{Describing bigram model}{section.12.2}
\BOOKMARK [3][-]{subsubsection.12.2.2.1}{State transition matrix}{subsection.12.2.2}
\BOOKMARK [3][-]{subsubsection.12.2.2.2}{State transition graph}{subsection.12.2.2}
\BOOKMARK [3][-]{subsubsection.12.2.2.3}{Types of states and chains}{subsection.12.2.2}
\BOOKMARK [3][-]{subsubsection.12.2.2.4}{Learning transition probabilities}{subsection.12.2.2}
\BOOKMARK [2][-]{subsection.12.2.3}{Unique Stationary distribution .. of ergodic chains}{section.12.2}
\BOOKMARK [2][-]{subsection.12.2.4}{Mixing time of Ergodic chain}{section.12.2}
\BOOKMARK [3][-]{subsubsection.12.2.4.1}{Purpose, definition}{subsection.12.2.4}
\BOOKMARK [3][-]{subsubsection.12.2.4.2}{Coupling lemma}{subsection.12.2.4}
\BOOKMARK [3][-]{subsubsection.12.2.4.3}{Mixing time bound}{subsection.12.2.4}
\BOOKMARK [2][-]{subsection.12.2.5}{Straight line state transitions}{section.12.2}
\BOOKMARK [3][-]{subsubsection.12.2.5.1}{Gambler's winnings}{subsection.12.2.5}
\BOOKMARK [3][-]{subsubsection.12.2.5.2}{Queue}{subsection.12.2.5}
\BOOKMARK [1][-]{section.12.3}{Martingale .. wrt filtration}{chapter.12}
\BOOKMARK [2][-]{subsection.12.3.1}{Problem}{section.12.3}
\BOOKMARK [3][-]{subsubsection.12.3.1.1}{Example}{subsection.12.3.1}
\BOOKMARK [2][-]{subsection.12.3.2}{Properties}{section.12.3}
\BOOKMARK [2][-]{subsection.12.3.3}{Stopping time T}{section.12.3}
\BOOKMARK [2][-]{subsection.12.3.4}{Doob martingale}{section.12.3}
\BOOKMARK [2][-]{subsection.12.3.5}{Find expected running time of a game}{section.12.3}
\BOOKMARK [2][-]{subsection.12.3.6}{Concentration around starting value}{section.12.3}
\BOOKMARK [3][-]{subsubsection.12.3.6.1}{Applied to Doob Martingale}{subsection.12.3.6}
\BOOKMARK [3][-]{subsubsection.12.3.6.2}{Additive Bound for deviation from mean}{subsection.12.3.6}
\BOOKMARK [3][-]{subsubsection.12.3.6.3}{Additive deviation bound for sum of Poisson trial RV's}{subsection.12.3.6}
\BOOKMARK [1][-]{section.12.4}{n-gram model}{chapter.12}
\BOOKMARK [2][-]{subsection.12.4.1}{Model}{section.12.4}
\BOOKMARK [3][-]{subsubsection.12.4.1.1}{Subsequence/ prefix probabilities: notation}{subsection.12.4.1}
\BOOKMARK [3][-]{subsubsection.12.4.1.2}{Actual probability}{subsection.12.4.1}
\BOOKMARK [3][-]{subsubsection.12.4.1.3}{Markov assumption}{subsection.12.4.1}
\BOOKMARK [2][-]{subsection.12.4.2}{Estimation}{section.12.4}
\BOOKMARK [3][-]{subsubsection.12.4.2.1}{n and corpus size}{subsection.12.4.2}
\BOOKMARK [3][-]{subsubsection.12.4.2.2}{Rare words}{subsection.12.4.2}
\BOOKMARK [2][-]{subsection.12.4.3}{Smoothing}{section.12.4}
\BOOKMARK [1][-]{section.12.5}{Partially observed states}{chapter.12}
\BOOKMARK [2][-]{subsection.12.5.1}{Observations, states}{section.12.5}
\BOOKMARK [3][-]{subsubsection.12.5.1.1}{Use}{subsection.12.5.1}
\BOOKMARK [3][-]{subsubsection.12.5.1.2}{Applications}{subsection.12.5.1}
\BOOKMARK [2][-]{subsection.12.5.2}{Model classes}{section.12.5}
\BOOKMARK [3][-]{subsubsection.12.5.2.1}{Generative model of Pr\(X, L\)}{subsection.12.5.2}
\BOOKMARK [3][-]{subsubsection.12.5.2.2}{Model L given X}{subsection.12.5.2}
\BOOKMARK [2][-]{subsection.12.5.3}{Partially observed state chain}{section.12.5}
\BOOKMARK [3][-]{subsubsection.12.5.3.1}{Graphical model}{subsection.12.5.3}
\BOOKMARK [3][-]{subsubsection.12.5.3.2}{Representations}{subsection.12.5.3}
\BOOKMARK [3][-]{subsubsection.12.5.3.3}{Decoding/ filtering}{subsection.12.5.3}
\BOOKMARK [3][-]{subsubsection.12.5.3.4}{Online label distribution inference}{subsection.12.5.3}
\BOOKMARK [3][-]{subsubsection.12.5.3.5}{Past Label Distribution inference}{subsection.12.5.3}
\BOOKMARK [3][-]{subsubsection.12.5.3.6}{Learning given \(X, L\) examples}{subsection.12.5.3}
\BOOKMARK [3][-]{subsubsection.12.5.3.7}{Learning given observation samples X only}{subsection.12.5.3}
\BOOKMARK [2][-]{subsection.12.5.4}{k-gram HMM}{section.12.5}
\BOOKMARK [1][-]{section.12.6}{Decision process}{chapter.12}
\BOOKMARK [0][-]{chapter.13}{Continuous response variables' prediction}{part.3}
\BOOKMARK [1][-]{section.13.1}{Data preparation and assumptions}{chapter.13}
\BOOKMARK [1][-]{section.13.2}{Generalized linear model}{chapter.13}
\BOOKMARK [2][-]{subsection.13.2.1}{Linear models}{section.13.2}
\BOOKMARK [2][-]{subsection.13.2.2}{Generalization}{section.13.2}
\BOOKMARK [3][-]{subsubsection.13.2.2.1}{Log linear model}{subsection.13.2.2}
\BOOKMARK [3][-]{subsubsection.13.2.2.2}{Logistic model}{subsection.13.2.2}
\BOOKMARK [3][-]{subsubsection.13.2.2.3}{Perceptron: step function}{subsection.13.2.2}
\BOOKMARK [1][-]{section.13.3}{Multi-layer generalized linear model}{chapter.13}
\BOOKMARK [2][-]{subsection.13.3.1}{Model}{section.13.3}
\BOOKMARK [3][-]{subsubsection.13.3.1.1}{Component names}{subsection.13.3.1}
\BOOKMARK [3][-]{subsubsection.13.3.1.2}{Activation function}{subsection.13.3.1}
\BOOKMARK [3][-]{subsubsection.13.3.1.3}{Visualization as a network}{subsection.13.3.1}
\BOOKMARK [3][-]{subsubsection.13.3.1.4}{Nomenclature}{subsection.13.3.1}
\BOOKMARK [2][-]{subsection.13.3.2}{Connection to other models}{section.13.3}
\BOOKMARK [2][-]{subsection.13.3.3}{Model training}{section.13.3}
\BOOKMARK [3][-]{subsubsection.13.3.3.1}{Gradient finding}{subsection.13.3.3}
\BOOKMARK [3][-]{subsubsection.13.3.3.2}{Weight initialization}{subsection.13.3.3}
\BOOKMARK [2][-]{subsection.13.3.4}{Flexibility}{section.13.3}
\BOOKMARK [2][-]{subsection.13.3.5}{Disadvantages}{section.13.3}
\BOOKMARK [1][-]{section.13.4}{Deep belief network}{chapter.13}
\BOOKMARK [-1][-]{part.4}{References}{}
\BOOKMARK [0][-]{section*.69}{Bibliography}{part.4}
