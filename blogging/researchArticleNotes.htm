<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Strict//EN">
<html>
  <head>
    <meta name="generator" content=
    "HTML Tidy for Linux/x86 (vers 1st October 2002), see www.w3.org">
    <meta content="text/html; charset=ISO-8859-1" http-equiv=
    "content-type">
    <title>Research article notes</title>
    <link rel="stylesheet" href="../../contentStylesheet.css" type=
    "text/css">
  </head>
  <body>
    <h4>On agnostic learning of parities, monomials and halfspaces:
    by FGKP.</h4>
    <h4>Depth with which this was read, progress:</h4>
    <p>Focusing sharply on sections 2 and 3. 3 hours ( 14 - 13
    pages ).<br>
    </p>
    <h4>Problems:</h4>
    What can you do with a noisy parity learner? Can you
    agnostically learn parities? Can you learn noisy DNF's?<br>
     
    <h5>Motivating real life scenarios:</h5>
    <h4>The Model:</h4>
    <h5>Terms and Variables used:</h5>
    <p>$\Del_D(f,h) = Pr_D[f \neq h]; \Del_D(f,C) = \inf_{h \in C}
    \set{\Del_D(f,h)}$. Agnostic learning: (x, b) drawn from D
    actually consistent with f(x). Generalized agnostic
    learning:(x, b) drawn from D not necessarily consistent with
    any f; $\Del_D(f,C)$ represents the best any $c \in C$ can
    do.</p>
    <p>Weakly agnostic alg: Weaker guarantees about error of h:
    $2\eta$ or $\eta + .2$.</p>
    <p>a is the index of $\hat{f}(a)$.</p>
    <p>$tf:\set{0, 1}^n \to [-1, 1]$ for $\pm 1$ boolean f:
    $t.f(x)$.</p>
    <p>Probabilistic oracle O(f) for any fn $f:\set{0, 1}^n \to
    [-1, 1]$: produces (x, b) where $x \distr U$, b is $\pm 1$
    binary RV with mean f(x). For boolean f, this is UPAC oracle
    EX(f, U).</p>
    A-projection of f: $f_A = \sum_{a \in \set{0, 1}^n: Aa = 1^m}
    \hat{f}(a)p_a(x)$.<br>
     
    <h4>Results, methods and ideas:</h4>
    <p>O(tf) is an oracle for f(x) with random noise of rate
    $1/2-t/2$: simply see how to make binary RV b with mean
    tE[f].</p>
    <p>Statistical distance between outputs of O(f) and O(g) is
    bounded by $\norm{f-g}$: If Fourier spectra close, outputs
    close.</p>
    <ul>
      <li>Pr that O(f) gives (x, 1) is (1 + f(x))/2. So,
      Statistical distance translates to $E_x[|f(x)-g(x)|]$. Then
      use Jensen's inequality.</li>
    </ul>
    <p>$f_A(x) = E_{p \in \set{0,1}^m}[f(x \xor A^T p)p_{1^m}(p)]$.
    Thus,&nbsp;Can simulate $O(f_A)$ with O(f).</p>
    <ul>
      <li>Use $p_a(x) = (-1)^{a^Tx}$ (<span style=
      "font-weight: bold;">Good trick</span>), observe that
      $p_a(A^T p) = p_{Aa}(p)$, $p_a(x \xor A^T p) = p_a(x)p_a(A^T
      p)$.</li>
      <li>Start with RHS,&nbsp;Use Fourier expansion of $f(x \xor
      A^T p)$, use linearity of expectations, get LHS.</li>
      <li>Can simulate $O(f_A)$ with O(f): Sample random p, get (x,
      b) from O(f), return (x + A^Tp, b.p_{1^m}(p)).</li>
    </ul>
    <p>Take [-1, 1] ranged f, $s \neq 0$, random A: with
    probability $2^{-m-1}$, $\hat{f}_A(s) = \hat{f}(s)$ and
    $\sum_{a \neq s}\hat{f_A}(a)^2 \leq 2^{-m+1} \norm{f}^2$.</p>
    <ul>
      <li>$\hat{f}_A(s) = \hat{f}(s)$ holds when $As = 1^m$, this
      has probability $2^{-m}$:&nbsp;.</li>
      <li>$E[\sum_{a \in \set{0,1}^n - s}\hat{f_A}(a)^2 | As = 1^m]
      \leq 2^{-m} \norm{f}^2$ by linearity of expectations. Then,
      get second result by Markov inequality.</li>
    </ul>
    <h5>Learnability results:</h5>
    <p>All positive agnostic learning results applied to
    generalized agnostic learning model.</p>
    <ul>
      <li>Results show agnostic learning over some distribution
      induced by O(f).</li>
    </ul>
    <br>
     
    <p>Under U, let alg A learn parities over k vars for every
    $\eta &lt; 1/2$ in time $T(n,k, (1-2\eta)^-1)$ and S(n,k,
    (1-2\eta)^-1)$ examples; then, given access to O(f) weak parity
    learner WP-R learns t-heavy, k-degree Fourier coefficient s for
    every [-1, 1] ranged fn f whp. Its run time is $O(T(n, k,
    1/t)S^2(n,k,1/t))$, sample complexity is $O(S^3(n,k,1/t))$.</p>
    <ul>
      <li>Let $S = (S(n,k,1/t)$. Pick random matrix A with $m = 2
      \log S +3$ rows. Take O(f), simulate $O(f_A)$, run A to get
      parity r, output r if r is t/2 heavy.</li>
      <li>With prob&nbsp;$2^{-m-1}$: $f_A$ has s as
      coefficient,&nbsp;$\sum_{a \neq s}\hat{f_A}(a)^2 \leq
      2/(4S^2)$.</li>
      <li>Statistical distance between $O(f_A(x))$ and
      $O(\hat{f}(A)(s)p_s(x))$ is at most 1/(2S). The variation
      distance between 2 distributions can be thought of as $\max_A
      D_1(A) - D_2(A)$. So, using any sample, the probability that
      alg A notices difference between the 2 distributions is
      1/(2S).A uses S samples. [ <span style=
      "font-weight: bold;">Good observation.</span>] So,
      probability that A notices difference between these is at
      most 1/2. The latter is just the noisy parity oracle.</li>
      <li>Note: f has at most $u = 1/t^2$ t-heavy coefficients: so,
      by coupon collector, with $u \log u$ repetitions, identify
      all t-heavy coefficients.</li>
    </ul>
    <br>
     
    <p>Agnostically learning parities reduces to learning parities
    with random classification noise.</p>
    <ul>
      <li>Take f, a parity $p_s$ with adversarial noise $\eta$;
      $\hat{f}(s) = E[f p_s] = 1 - 2\eta$; so, with nnp, for some
      $f_A$, get oracle $O(f_A)$ close to noisy oracle
      $O(\hat{f}(a)(s)p_s(x))$.</li>
      <li>Note: Then use BKW noisy parity alg to get
      $2^{O(\frac{n}{\log n})}$ agnostic alg.</li>
      <li>So agnostically learning parities equivalent to learning
      significant Fourier coefficient of a Boolean function.</li>
    </ul>
    <p>Agnostic learning of parities equivalent to decoding random
    linear codes from adversarial errors.</p>
    <ul>
      <li>xG = c; make H in $N(G^T)$; (c+e)H = eH. Finding e from
      eH is equivalent to learning noisy parities.</li>
      <li>So, this is NP hard.</li>
      <li>So, can make decoding alg for adversarial noise from
      decoding alg for random noise.</li>
    </ul>
    <p>Learning DNF expressions reduces to learning noisy parities
    of only log n variables.</p>
    <ul>
      <li>For every distribution D, $\exists$ parity $p_a$ with
      $|E_D[f p_a]| \geq (2s+1)^-1$ with $wt(a)\leq
      \log((2s+1)\norm{2^n D}_\infty)$. So, can weakly learn f by
      finding p_a correlated with f(x)D(x) under U. So, scale it to
      get $f(x)D'(x) = f(x)D(x)/\norm{D}_\infty)$ with range [-1,
      1]. Thence get oracle O(D'(x)f(x)) and use agnostic parity
      learner.</li>
      <li>Then use a p-smooth booster.</li>
      <li><span style="text-decoration: underline;">Note</span>:
      Can make this resilient to noise.</li>
    </ul>
    <p>Learning k-juntas: A clean formulation of the problem of
    efficient learning in the presence of irrelevant attributes.
    Can reduce this to noisy parity problem. Can take k-PARITY
    learner which works with noise $\eta$ in time $T(n,k,
    (1-2\eta)^{-1})$; Get k-junta learner with time complexity
    $O(k2^{2k}T(n,k,2^{k-1}))$: k-junta learning reduced to special
    case with blow up in noise.</p>
    <ul>
      <li>k-junta has at most $2^k$ non 0 coefficients. Each of
      them is $2^{-k+1}$ heavy: Make two equally high stacks
      unequal. [<span style="font-weight: bold;">Good
      observation!</span>]</li>
      <li>Now, use WP-R O(k2^{2k}) times to learn full spectrum.
      Noise parameter of the oracle $O(f_a p_a)$ will be
      $&lt;2^{-1} - 2^{-k}$. While running WP-R, no need to check
      weight.</li>
      <li>Get algorithm to find significant Fourier coefficients
      from random samples from agnostic parity learning alg.</li>
      <li><span style="text-decoration: underline;">Note</span>:
      Noise tolerance: Oracle for f with noise $\eta$ is same as
      $O((1-2\eta)f)$. Use this oracle in above alg to get oracle
      $O((1-2\eta)f_a p_a)$, use it as above.</li>
    </ul>
    <br>
     
    <p>Hardness results for monomials and halfspaces.</p>
    <h4>Assumptions:</h4>
    <br>
     
    <h4>What could have been tried to yield equivalent
    results:</h4>
    <h4>Open problems:</h4>
    <br>
     
    <h4>Interesting facts and results from elsewhere:</h4>
    <p>Agnostic learning model can be thought of as a model of
    adversarial classification noise: examples are labelled
    according to $f \in C$, but labels corrupted for $\eta$
    fraction. Not same as: Learning with Malicious errors
    (Valiant): Both label and point may be corrupted, changing
    D.</p>
    <p>Designing agnostic learning algorithms notoriously hard,
    very few positive results known.</p>
    <p>Proper vs improper agnostic learning.</p>
    <br>
     
    <p>BKW presented an algorithm for learning parity functions on
    n variables in the presence of random noise in time
    $2^{O(\frac{n}{\log n})}$ for any constant $\eta$, for any
    distribution D.</p>
    <p>Learning k-PARITY with random noise: Measure errors of all
    $O(n^k)$ h over a random sample set of size
    $O(\frac{1}{1-2\eta} k \log n)$. Using the Hoeffding bound.</p>
    <p>Learn DNF under U in $O(n^{\log(s/\eps)})$ (Verbeurgt):
    Collect all terms of size $log(s/\eps)$ consistent with target
    DNF: use feature expansion, disjunction learning winnow
    alg.</p>
    <h4>Comments on writing style:</h4>
    <p><br>
    </p>
    <h4>Questions:</h4>
    <p><br>
    </p>
  </body>
</html>
