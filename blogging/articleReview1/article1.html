 \maketitle
<p>


<p>

<b>1. Depth with which this was read, progress </b>

<p>

 4 hours ( 10 - 3 pages ).
<p>


<p>

<b>2. Problems </b>

<p>


<p>
What is the precise correspondence between hard to learn C and cryptographic primitives? Do representation independent hardness of learning results require cryptographic assumptions?
<p>
How to modify learning definitions to make hardness of learning assumption equivalent to a cryptographic assumption? How to use such assumptions to make cryptographic primitives?
<p>


<p>

<b>3. Motivating real life scenarios </b>

<p>


<p>


<p>

<b>4. The Model </b>

<p>

 

<p>

<b>  4.1. Terms and Variables used </b>

<p>


<p>
Expansion of a pseudorandom bit generator.
<p>
$latex {F_n}&fg=000000$ : functions over n-dim hypercube. $latex {F = \union F_n}&fg=000000$. Representation scheme: $latex {(R_n, E_n)}&fg=000000$ where every $latex {s \in R_n}&fg=000000$ is r(n) sized representation of $latex {E_n(s) \in F_n}&fg=000000$. $latex {(R, E) = \union \set{(R_n, E_n)}}&fg=000000$.
<p>
Distribution over $latex {F_n}&fg=000000$ or $latex {R_n: P_n}&fg=000000$. Their ensemble is P. Distribution over $latex {\set{0,1}^{n}: D_{n}}&fg=000000$. Their ensemble is D.
<p>


<p>

<b>5. Results, methods and ideas </b>

<p>


<p>


<p>

<b>  5.1. The average case model of learning </b>

<p>


<p>
Modify learning definitions to make hardness of learning assumption equivalent to a cryptographic assumption.
<p>
\subsubsection{Motivation} Take a hard to learn C. There may be some alg A which learns all but a small scattered fraction of C. If hardness of learning C were a cryptographic assumption, A would be a good attacker. So, use average case model of learning.
<p>


<p>

<b>  5.2. Learnability and Cryptographic results </b>

<p>


<p>
\subsubsection{Strong correspondence between hardness of learning and cryptography}
<p>
\subsubsection{Generic transformation of C hard to weakly learn in the average case model into cryptographically secure pseudorandom bit generator}
<p>
Ckt depth depends on ckt depth of c in C and ckt required for generating hard distribution. Evidence that representation independent hardness of learning results require cryptographic assumptions. Is this truly the case?
<p>
If C hard to weakly learn in the average case model, even with mq, get pseudorandom bit generator with better expansion.
<p>
\subsubsection{If C hard to strongly learn in the average case model, get one way circuits}
<p>
Parallelism preserved: Faster one way functions imply faster key sharing protocols.
<p>
\subsubsection{If C hard to weakly learn in the average case model, get CPA secure private key crypto system}
<p>
\subsubsection{Simpler construction of pseudorandom generator using the learning parity with noise assumption}
<p>


<p>

<b>6. Assumptions </b>

<p>


<p>


<p>

<b>7. What could have been tried to yield equivalent results </b>

<p>


<p>


<p>

<b>8. Open problems </b>

<p>


<p>


<p>

<b>9. Interesting facts and results from elsewhere </b>

<p>


<p>
Pseudorandom bit generators can be turned into one way functions; and vice versa. But, sometimes, these don't preserve the circuit depth. How is this done?
<p>
The learning parities with noise assumption was used by Vaikuntanathan et al to build the first IBE system which does not require bilinear maps.
<p>


<p>

<b>10. Comments on writing style </b>

<p>


<p>


<p>

<b>11. Questions </b>

<p>


<p>
