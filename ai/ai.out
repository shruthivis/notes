\BOOKMARK [0][-]{section*.1}{Contents}{}
\BOOKMARK [-1][-]{part.1}{Introduction}{}
\BOOKMARK [0][-]{chapter.1}{Themes}{part.1}
\BOOKMARK [1][-]{section.1.1}{Tasks which may befall the intelligent agent}{chapter.1}
\BOOKMARK [1][-]{section.1.2}{Characterization of research effort}{chapter.1}
\BOOKMARK [0][-]{chapter.2}{The intelligent agent}{part.1}
\BOOKMARK [1][-]{section.2.1}{Interaction model}{chapter.2}
\BOOKMARK [1][-]{section.2.2}{Environment traits}{chapter.2}
\BOOKMARK [1][-]{section.2.3}{Rationality}{chapter.2}
\BOOKMARK [2][-]{subsection.2.3.1}{Attitude towards risk}{section.2.3}
\BOOKMARK [1][-]{section.2.4}{Models of rational agents}{chapter.2}
\BOOKMARK [2][-]{subsection.2.4.1}{Performance measures}{section.2.4}
\BOOKMARK [-1][-]{part.2}{Basic procedures}{}
\BOOKMARK [0][-]{chapter.3}{Search graph for goal node}{part.2}
\BOOKMARK [0][-]{chapter.4}{Constraint satisfaction problems}{part.2}
\BOOKMARK [1][-]{section.4.1}{As an uninformed search problem}{chapter.4}
\BOOKMARK [1][-]{section.4.2}{Local search for CSPs}{chapter.4}
\BOOKMARK [0][-]{chapter.5}{Adverserial search / games}{part.2}
\BOOKMARK [1][-]{section.5.1}{Minmax algorithm}{chapter.5}
\BOOKMARK [0][-]{chapter.6}{Inference}{part.2}
\BOOKMARK [0][-]{chapter.7}{Stochastic control processes}{part.2}
\BOOKMARK [1][-]{section.7.1}{Reinforcement learning setting}{chapter.7}
\BOOKMARK [2][-]{subsection.7.1.1}{Interaction with the environment}{section.7.1}
\BOOKMARK [2][-]{subsection.7.1.2}{Policy learning}{section.7.1}
\BOOKMARK [3][-]{subsubsection.7.1.2.1}{Sub-problems}{subsection.7.1.2}
\BOOKMARK [2][-]{subsection.7.1.3}{Reward learning}{section.7.1}
\BOOKMARK [3][-]{subsubsection.7.1.3.1}{Non triviality criteria}{subsection.7.1.3}
\BOOKMARK [3][-]{subsubsection.7.1.3.2}{Motivation}{subsection.7.1.3}
\BOOKMARK [2][-]{subsection.7.1.4}{Reinforcement learning in Animals}{section.7.1}
\BOOKMARK [3][-]{subsubsection.7.1.4.1}{Conditioning}{subsection.7.1.4}
\BOOKMARK [2][-]{subsection.7.1.5}{Learning by simulation}{section.7.1}
\BOOKMARK [1][-]{section.7.2}{Markov decision process \(MDP\)}{chapter.7}
\BOOKMARK [2][-]{subsection.7.2.1}{Abstract problem}{section.7.2}
\BOOKMARK [3][-]{subsubsection.7.2.1.1}{Limited dependence on history}{subsection.7.2.1}
\BOOKMARK [3][-]{subsubsection.7.2.1.2}{Representation, notation}{subsection.7.2.1}
\BOOKMARK [2][-]{subsection.7.2.2}{Policy}{section.7.2}
\BOOKMARK [3][-]{subsubsection.7.2.2.1}{Long term reward}{subsection.7.2.2}
\BOOKMARK [3][-]{subsubsection.7.2.2.2}{Weighted sum: state values}{subsection.7.2.2}
\BOOKMARK [3][-]{subsubsection.7.2.2.3}{Best policy}{subsection.7.2.2}
\BOOKMARK [3][-]{subsubsection.7.2.2.4}{Solution techniques}{subsection.7.2.2}
\BOOKMARK [2][-]{subsection.7.2.3}{Dealing with large state spaces}{section.7.2}
\BOOKMARK [1][-]{section.7.3}{Partially observable Markov decision process \(POMDP\)}{chapter.7}
\BOOKMARK [2][-]{subsection.7.3.1}{Problem setting}{section.7.3}
\BOOKMARK [3][-]{subsubsection.7.3.1.1}{Observations}{subsection.7.3.1}
\BOOKMARK [3][-]{subsubsection.7.3.1.2}{Simplified rewards}{subsection.7.3.1}
\BOOKMARK [2][-]{subsection.7.3.2}{Belief states}{section.7.3}
\BOOKMARK [0][-]{chapter.8}{Planning}{part.2}
\BOOKMARK [1][-]{section.8.1}{Specifying decision algorithm}{chapter.8}
\BOOKMARK [2][-]{subsection.8.1.1}{Rule based vs utility computation}{section.8.1}
\BOOKMARK [0][-]{chapter.9}{Computer vision}{part.2}
\BOOKMARK [1][-]{section.9.1}{SLAM \(Simultaneous localization and mapping\) problem}{chapter.9}
\BOOKMARK [2][-]{subsection.9.1.1}{Tools and approaches}{section.9.1}
\BOOKMARK [2][-]{subsection.9.1.2}{Features}{section.9.1}
\BOOKMARK [2][-]{subsection.9.1.3}{Feature detection}{section.9.1}
\BOOKMARK [3][-]{subsubsection.9.1.3.1}{Scale invariant feature transform \(SIFT\)}{subsection.9.1.3}
\BOOKMARK [2][-]{subsection.9.1.4}{Feature matching}{section.9.1}
\BOOKMARK [1][-]{section.9.2}{Visual object recognition}{chapter.9}
\BOOKMARK [2][-]{subsection.9.2.1}{Eigenfaces}{section.9.2}
\BOOKMARK [0][-]{section*.5}{Bibliography}{part.2}
